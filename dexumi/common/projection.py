import math
import warnings
from functools import partial

import numpy as np
import torch
import torch.nn.functional as F
from timm.layers import get_norm_layer
from torch import nn


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2,
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


class ProjectionHead(nn.Module):
    def __init__(
        self,
        in_dim,
        out_dim,
        use_bn=False,
        nlayers=3,
        hidden_dim=2048,
        apply_norm_layer=False,
        norm_layer=None,
    ):
        super().__init__()
        nlayers = max(nlayers, 1)
        if nlayers == 1:
            self.mlp = nn.Linear(in_dim, out_dim)
        else:
            layers = [nn.Linear(in_dim, hidden_dim)]
            if use_bn:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.GELU())
            for _ in range(nlayers - 2):
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                if use_bn:
                    layers.append(nn.BatchNorm1d(hidden_dim))
                layers.append(nn.GELU())
            layers.append(
                nn.Linear(
                    hidden_dim,
                    out_dim,
                )
            )
            self.mlp = nn.Sequential(*layers)
        self.apply_norm_layer = apply_norm_layer
        if self.apply_norm_layer:
            norm_layer = get_norm_layer(norm_layer) or partial(nn.LayerNorm, eps=1e-6)
            self.norm_layer = norm_layer(out_dim)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.mlp(x)
        if self.apply_norm_layer:
            x = self.norm_layer(x)
        return x


class Proj(nn.Module):
    def __init__(
        self,
        in_dim,
        out_dim,
        bias=False,
    ) -> None:
        super().__init__()
        self.proj = nn.Linear(in_dim, out_dim, bias=bias)
        self.apply(self._init_weights)

    def forward(self, x):
        return self.proj(x)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)


class Identity(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.proj = nn.Identity()

    def forward(self, x):
        return self.proj(x)


# copy from dino; rename from DinoHead to ProjectionHead
class PrototypeHead(nn.Module):
    def __init__(
        self,
        in_dim,
        out_dim,
        use_bn=False,
        norm_last_layer=True,
        nlayers=3,
        hidden_dim=2048,
        bottleneck_dim=256,
    ):
        super().__init__()
        nlayers = max(nlayers, 1)
        if nlayers == 1:
            self.mlp = nn.Linear(in_dim, bottleneck_dim)
        else:
            layers = [nn.Linear(in_dim, hidden_dim)]
            if use_bn:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.GELU())
            for _ in range(nlayers - 2):
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                if use_bn:
                    layers.append(nn.BatchNorm1d(hidden_dim))
                layers.append(nn.GELU())
            layers.append(nn.Linear(hidden_dim, bottleneck_dim))
            self.mlp = nn.Sequential(*layers)
        self.apply(self._init_weights)
        self.last_layer = nn.utils.weight_norm(
            nn.Linear(bottleneck_dim, out_dim, bias=False)
        )
        self.last_layer.weight_g.data.fill_(1)
        if norm_last_layer:
            self.last_layer.weight_g.requires_grad = False

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.mlp(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x


class CenterNSharpen(nn.Module):
    def __init__(
        self,
        warmup_temp,
        temp,
        warmup_temp_epochs,
        nepochs,
        out_dim,
        center_momentum=0.9,
    ):
        super().__init__()
        self.center_momentum = center_momentum
        self.register_buffer("center", torch.zeros(1, out_dim))
        self.temp_schedule = np.concatenate(
            (
                np.linspace(warmup_temp, temp, warmup_temp_epochs),
                np.ones(nepochs - warmup_temp_epochs) * temp,
            )
        )

    def forward(self, x, epoch):
        temp = self.temp_schedule[epoch]
        x = F.softmax((x - self.center) / temp, dim=-1)
        return x


class FiLMLayer(nn.Module):
    def __init__(self, in_dim, out_dim):
        super(FiLMLayer, self).__init__()
        # Define layers to generate gamma and beta parameters
        self.gamma_layer = nn.Linear(in_dim, out_dim)
        self.beta_layer = nn.Linear(in_dim, out_dim)

        # Initialize weights for gamma generation to be close to 1
        nn.init.normal_(self.gamma_layer.weight, mean=0.0, std=0.02)
        nn.init.constant_(self.gamma_layer.bias, 1.0)  # Biasing gamma towards 1

        # Initialize weights for beta generation to be close to 0
        nn.init.normal_(self.beta_layer.weight, mean=0.0, std=0.02)
        nn.init.constant_(self.beta_layer.bias, 0.0)  # Starting with minimal shift

    def forward(self, x, condition):
        # Generate gamma and beta from the input_tensor
        gamma = self.gamma_layer(condition)
        beta = self.beta_layer(condition)

        # Apply FiLM modulation: target_tensor * gamma + beta
        modulated_tensor = x * gamma + beta
        return modulated_tensor


class NumericToCLIP(nn.Module):
    def __init__(
        self,
        n_token,
        out_dim,
    ):
        super().__init__()
        self.n_token = n_token
        self.out_dim = out_dim

    def forward(self, x):
        B, M = x.shape
        # Repeat each element out_dim times
        x = x.unsqueeze(-1).repeat(1, 1, self.out_dim)

        return x

    def to(self, *args, **kwargs):
        # Override to() method to handle dtype changes
        module = super().to(*args, **kwargs)
        if len(args) > 0 and isinstance(args[0], torch.dtype):
            module.dtype = args[0]
        elif "dtype" in kwargs:
            module.dtype = kwargs["dtype"]
        return module


class NumericToCLIPV2(nn.Module):
    def __init__(self, n_token, out_dim):
        super().__init__()
        self.n_token = n_token
        self.out_dim = out_dim
        self.linear = nn.Linear(1, out_dim)
        self._dtype = torch.float32  # Add default dtype

        # Initialize all parameters to zero
        with torch.no_grad():
            torch.nn.init.zeros_(self.linear.weight)
            torch.nn.init.zeros_(self.linear.bias)

    @property
    def dtype(self):
        # Return dtype of parameters if they exist, otherwise return stored dtype
        if hasattr(self, "linear") and self.linear is not None:
            return next(self.parameters()).dtype
        return self._dtype

    def forward(self, x):
        B, M = x.shape
        # Pad or truncate to n_token length
        if M < self.n_token:
            padding = torch.zeros(B, self.n_token - M, device=x.device, dtype=x.dtype)
            x = torch.cat([x, padding], dim=1)
        elif M > self.n_token:
            raise ValueError(
                f"Input sequence length {M} is greater than n_token {self.n_token}"
            )

        # Project through linear layer
        x = x.view(B * self.n_token, 1)  # Shape: (B*n_token, 1)
        x = self.linear(x)  # Shape: (B*n_token, out_dim)
        x = x.view(B, self.n_token, self.out_dim)  # Shape: (B, n_token, out_dim)
        return x

    def to(self, *args, **kwargs):
        # Override to() method to handle dtype changes
        if len(args) > 0 and isinstance(args[0], torch.dtype):
            self._dtype = args[0]
        elif "dtype" in kwargs:
            self._dtype = kwargs["dtype"]
        return super().to(*args, **kwargs)
